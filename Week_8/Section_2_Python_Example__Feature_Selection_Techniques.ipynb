{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_8/Section_2_Python_Example__Feature_Selection_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 2: Python example - feature selection techniques"
      ],
      "metadata": {
        "id": "PuuSi4Oq6Pji"
      },
      "id": "PuuSi4Oq6Pji"
    },
    {
      "cell_type": "markdown",
      "id": "346e1d29",
      "metadata": {
        "id": "346e1d29"
      },
      "source": [
        "Feature selection is a critical step in preparing your data for machine learning models. It involves identifying the most significant features that contribute positively to the predictive power of the model. This section provides practical Python examples demonstrating various feature selection techniques using libraries such as scikit-learn, which offers a comprehensive suite of methods for automated feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c510a94",
      "metadata": {
        "id": "1c510a94"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bd10871",
      "metadata": {
        "id": "8bd10871"
      },
      "source": [
        "Before implementing feature selection techniques, ensure your Python environment includes scikit-learn. If not already installed, you can add it via pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7483e952",
      "metadata": {
        "id": "7483e952"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5e7d66",
      "metadata": {
        "id": "ff5e7d66"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40491c3f",
      "metadata": {
        "id": "40491c3f"
      },
      "source": [
        "Along with scikit-learn, we will use Pandas for handling data and NumPy for any additional numerical operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a25d28",
      "metadata": {
        "id": "18a25d28"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b72d291",
      "metadata": {
        "id": "7b72d291"
      },
      "source": [
        "3. Preparing the Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efdf424c",
      "metadata": {
        "id": "efdf424c"
      },
      "source": [
        "For this example, letâ€™s use a synthetic dataset that simulates customer data for a bank marketing campaign:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb5b54f",
      "metadata": {
        "id": "8cb5b54f"
      },
      "outputs": [],
      "source": [
        "# Create a synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_repeated=0, n_classes=2, random_state=42, shuffle=False)\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(20)])\n",
        "df['Target'] = y\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('Target', axis=1), df['Target'], test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f121a5ba",
      "metadata": {
        "id": "f121a5ba"
      },
      "source": [
        "4. Feature Selection with Univariate Statistical Tests:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b37e0d13",
      "metadata": {
        "id": "b37e0d13"
      },
      "source": [
        "Univariate statistical tests can be used to select the number of best features that have the strongest relationship with the output variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ccca0cc",
      "metadata": {
        "id": "9ccca0cc"
      },
      "outputs": [],
      "source": [
        "# Apply SelectKBest class to extract top 10 best features\n",
        "best_features = SelectKBest(score_func=f_classif, k=10)\n",
        "fit = best_features.fit(X_train, y_train)\n",
        "df_scores = pd.DataFrame(fit.scores_)\n",
        "df_columns = pd.DataFrame(X_train.columns)\n",
        "# Concatenate dataframes for better visualization\n",
        "feature_scores = pd.concat([df_columns, df_scores], axis=1)\n",
        "feature_scores.columns = ['Feature', 'Score'] # Naming the dataframe columns\n",
        "print(feature_scores.nlargest(10, 'Score')) # Print 10 best features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9643ffc5",
      "metadata": {
        "id": "9643ffc5"
      },
      "source": [
        "5. Feature Selection Using Feature Importance from Model:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f35b229",
      "metadata": {
        "id": "5f35b229"
      },
      "source": [
        "You can also use an ensemble method like Random Forest to estimate the importance of features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e87044",
      "metadata": {
        "id": "49e87044"
      },
      "outputs": [],
      "source": [
        "# Training a random forest classifier\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "for f in range(X_train.shape[1]):\n",
        "  print(f\"{f + 1}. feature {indices[f]} ({importances[indices[f]]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce1efd6",
      "metadata": {
        "id": "cce1efd6"
      },
      "source": [
        "6. Evaluating Model Performance:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e612dac",
      "metadata": {
        "id": "2e612dac"
      },
      "source": [
        "To see how effective feature selection is, you can compare the performance of the model with and without feature selection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c23285",
      "metadata": {
        "id": "14c23285"
      },
      "outputs": [],
      "source": [
        "# Selecting features based on importance\n",
        "selected_features = X_train.columns[indices[:10]]\n",
        "# Rebuild model on selected features\n",
        "model.fit(X_train[selected_features], y_train)\n",
        "y_pred = model.predict(X_test[selected_features])\n",
        "# Check the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with selected features: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f07872",
      "metadata": {
        "id": "76f07872"
      },
      "source": [
        "7. Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d0bc90",
      "metadata": {
        "id": "d4d0bc90"
      },
      "source": [
        "These feature selection techniques demonstrate how reducing the number of features in your dataset can potentially improve the performance of your models, decrease overfitting, and enhance generalization. Using libraries like scikit-learn makes implementing these techniques straightforward, allowing data scientists to focus more on model optimization and less on manual feature selection. These methods provide a robust framework for automating the selection of the most informative features, ensuring that models are both efficient and effective."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}