{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_7/Section_6_Python_Example__Normalizing_and_Scaling_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1adac156",
      "metadata": {
        "id": "1adac156"
      },
      "source": [
        "#Section 6 - Python example: Normalizing and scaling data\n",
        "In the context of data preprocessing, normalization and scaling are crucial techniques used to standardize the range of independent variables or features of data. These methods are particularly important in machine learning, where they can significantly impact the performance of algorithms. This section provides a Python example demonstrating how to normalize and scale data using Scikit-learn, a powerful machine learning library that offers easy-to-use tools for these purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d18ef7b",
      "metadata": {
        "id": "7d18ef7b"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e691682b",
      "metadata": {
        "id": "e691682b"
      },
      "source": [
        "Before implementing normalization and scaling, ensure your Python environment includes Scikit-learn. If not already installed, it can be added via pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bdbe9a",
      "metadata": {
        "id": "61bdbe9a"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3bc7d2c",
      "metadata": {
        "id": "b3bc7d2c"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc151bba",
      "metadata": {
        "id": "cc151bba"
      },
      "source": [
        "Scikit-learn provides specific modules for preprocessing data. We'll also use Pandas for data manipulation and NumPy for any additional numerical operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437f5cc3",
      "metadata": {
        "id": "437f5cc3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2441f9d",
      "metadata": {
        "id": "b2441f9d"
      },
      "source": [
        "3. Creating Sample Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef46d4c7",
      "metadata": {
        "id": "ef46d4c7"
      },
      "source": [
        "Let's create a DataFrame with some sample data representing ages and incomes, which often require scaling due to their different ranges:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ae979c",
      "metadata": {
        "id": "00ae979c"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame\n",
        "data = pd.DataFrame({ 'Age': [25, 35, 45, 55, 20, 30, 40, 50, 60], 'Income': [50000, 60000, 70000, 80000, 35000, 45000, 55000, 65000, 75000] })\n",
        "print(\"Original Data:\\n\", data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d6ee19",
      "metadata": {
        "id": "59d6ee19"
      },
      "source": [
        "4. Applying Min-Max Scaling:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90166fac",
      "metadata": {
        "id": "90166fac"
      },
      "source": [
        "MinMaxScaler transforms features by scaling each feature to a given range, often [0, 1]. This is useful when parameters need to be on a positive scale and is common in algorithms that incorporate neural networks or require data within a bounded interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91775673",
      "metadata": {
        "id": "91775673"
      },
      "outputs": [],
      "source": [
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# Fit and transform the data\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "# Convert the array back to a DataFrame\n",
        "data_scaled = pd.DataFrame(data_scaled, columns=['Age', 'Income'])\n",
        "print(\"Data after Min-Max Scaling:\\n\", data_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b2fc31",
      "metadata": {
        "id": "63b2fc31"
      },
      "source": [
        "5. Applying Standardization (Z-score Normalization):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b352f2f",
      "metadata": {
        "id": "0b352f2f"
      },
      "source": [
        "StandardScaler removes the mean and scales each feature/variable to unit variance. This technique is less affected by outliers and is often used in clustering analyses and principal component analysis (PCA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39423634",
      "metadata": {
        "id": "39423634"
      },
      "outputs": [],
      "source": [
        "# Initialize the StandardScaler\n",
        "standard_scaler = StandardScaler()\n",
        "# Fit and transform the data\n",
        "data_standardized = standard_scaler.fit_transform(data)\n",
        "# Convert the array back to a DataFrame data_standardized = pd.DataFrame(data_standardized, columns=['Age', 'Income'])\n",
        "print(\"Data after Standardization:\\n\", data_standardized)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9ce8fe",
      "metadata": {
        "id": "9f9ce8fe"
      },
      "source": [
        "6. Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55dd09e",
      "metadata": {
        "id": "a55dd09e"
      },
      "source": [
        "Normalization and scaling are pivotal preprocessing techniques that help harmonize different features in a dataset, ensuring that each feature contributes equally to the development of machine learning models and that the model is not biased toward variables on larger scales. By using Scikit-learn's preprocessing tools, data scientists can easily implement these techniques, enhancing model accuracy and improving overall predictive performance. These methods are especially important in datasets where the variables differ significantly in their ranges or distributions, as is often the case in real-world scenarios."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}