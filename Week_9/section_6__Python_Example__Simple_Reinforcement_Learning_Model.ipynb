{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_9/section_6__Python_Example__Simple_Reinforcement_Learning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 6: python example - simple reinforcement learning model"
      ],
      "metadata": {
        "id": "QsNpwYG13F80"
      },
      "id": "QsNpwYG13F80"
    },
    {
      "cell_type": "markdown",
      "id": "cd3b074b",
      "metadata": {
        "id": "cd3b074b"
      },
      "source": [
        "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. This section provides a practical example of building a simple reinforcement learning model using Python, specifically implementing the Q-learning algorithm, a popular method for learning optimal policies in finite Markov decision processes (MDPs)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "648a220b",
      "metadata": {
        "id": "648a220b"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43f69ffc",
      "metadata": {
        "id": "43f69ffc"
      },
      "source": [
        "To get started with our reinforcement learning example, ensure Python is equipped with essential libraries. If not already installed, add them via pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3970e9",
      "metadata": {
        "id": "ab3970e9"
      },
      "outputs": [],
      "source": [
        "pip install numpy gym"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ef480c",
      "metadata": {
        "id": "b5ef480c"
      },
      "source": [
        "Gym by OpenAI is a toolkit for developing and comparing reinforcement learning algorithms. It provides a variety of environments that simulate different physical and virtual settings, which we will use to test our RL agent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc51db89",
      "metadata": {
        "id": "dc51db89"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b388da50",
      "metadata": {
        "id": "b388da50"
      },
      "source": [
        "Import the necessary libraries for creating the environment and performing numerical computations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe9287e",
      "metadata": {
        "id": "dfe9287e"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc577809",
      "metadata": {
        "id": "fc577809"
      },
      "source": [
        "3. Setting Up the RL Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6306a4b",
      "metadata": {
        "id": "a6306a4b"
      },
      "source": [
        "For this example, we'll use the \"FrozenLake-v0\" environment from Gym, which represents a grid world where an agent must navigate across a frozen lake without falling into holes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4f8b53",
      "metadata": {
        "id": "4c4f8b53"
      },
      "outputs": [],
      "source": [
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False) # 'is_slippery: False' makes the environment deterministic\n",
        "env.reset() # Reset the environment to the initial state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef743d26",
      "metadata": {
        "id": "ef743d26"
      },
      "source": [
        "4. Implementing Q-Learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b827281",
      "metadata": {
        "id": "9b827281"
      },
      "source": [
        "Q-learning is an off-policy learner that seeks to find the best action to take given the current state. It does this by updating Q-values (action-value pairs) using the equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da180aa7",
      "metadata": {
        "id": "da180aa7"
      },
      "source": [
        "Q(s,a)←Q(s,a)+α[r+γmaxa′​Q(s′,a′)−Q(s,a)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42062344",
      "metadata": {
        "id": "42062344"
      },
      "source": [
        "where:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace3a722",
      "metadata": {
        "id": "ace3a722"
      },
      "source": [
        "*  **s** is the current state,\n",
        "*  **a** is the current action,\n",
        "*  **r** is the reward received after executing the action,\n",
        "*  **s′** is the new state after action aa,\n",
        "*  **α** is the learning rate,\n",
        "*  **γ** is the discount factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ceea59",
      "metadata": {
        "id": "e0ceea59"
      },
      "outputs": [],
      "source": [
        "# Initialize the Q-table to zero\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.8  # Learning rate\n",
        "gamma = 0.95  # Discount factor\n",
        "num_episodes = 2000\n",
        "\n",
        "# Q-learning algorithm\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose an action by greedily picking from Q table (with noise)\n",
        "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))\n",
        "\n",
        "        # Take the action and observe the new state and reward\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q-Table using the Bellman equation\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "# Display the Q-table\n",
        "print(\"Q-table:\")\n",
        "print(Q)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83524d7",
      "metadata": {
        "id": "e83524d7"
      },
      "source": [
        "5. Testing the Agent:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447989c6",
      "metadata": {
        "id": "447989c6"
      },
      "source": [
        "After training, you can test the agent by making it navigate the environment using the learned Q-values. Please note this will not work in colab - you are best to run this in your own Python environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "for _ in range(1000):  # Limit the number of steps\n",
        "    action = np.argmax(Q[state, :])  # Choose the best action from the Q-table\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    env.render()  # Display the environment\n",
        "    if done:\n",
        "        break\n",
        "    state = new_state"
      ],
      "metadata": {
        "id": "poDk5Qmt6FGN"
      },
      "id": "poDk5Qmt6FGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "94a76ab5",
      "metadata": {
        "id": "94a76ab5"
      },
      "source": [
        "6. Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aefd887f",
      "metadata": {
        "id": "aefd887f"
      },
      "source": [
        "This simple example demonstrates the implementation of a Q-learning agent in a deterministic version of the FrozenLake environment. Reinforcement learning, particularly Q-learning, offers a robust framework for teaching agents to perform complex tasks by learning optimal policies. While the example is basic, the principles can be extended to more complex and realistic environments. As such, reinforcement learning continues to be a valuable approach in the field of artificial intelligence, providing tools to solve diverse decision-making problems."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}