{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_9/Section_8__Python_Example__Basic_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 8: Python example - basic text analysis"
      ],
      "metadata": {
        "id": "LKsFoeVf67-r"
      },
      "id": "LKsFoeVf67-r"
    },
    {
      "cell_type": "markdown",
      "id": "ec2f2126",
      "metadata": {
        "id": "ec2f2126"
      },
      "source": [
        "Text analysis is a fundamental aspect of Natural Language Processing (NLP) that involves processing text to understand its structure, meaning, and intent. In this section, we will use Python to demonstrate basic text analysis techniques, including tokenization, part-of-speech tagging, and named entity recognition. We'll use the Natural Language Toolkit (NLTK), a widely used library for NLP in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7bb2b47",
      "metadata": {
        "id": "d7bb2b47"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d6a0b4",
      "metadata": {
        "id": "54d6a0b4"
      },
      "source": [
        "To start with text analysis in Python, you'll need the NLTK library, which provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Install NLTK using pip if it's not already installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc57fb0",
      "metadata": {
        "id": "6dc57fb0"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0645c4d",
      "metadata": {
        "id": "a0645c4d"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9015f669",
      "metadata": {
        "id": "9015f669"
      },
      "source": [
        "After installation, import NLTK and download the necessary datasets and models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d12157",
      "metadata": {
        "id": "06d12157"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e45d66b",
      "metadata": {
        "id": "5e45d66b"
      },
      "source": [
        "3. Tokenization:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed86e1ef",
      "metadata": {
        "id": "ed86e1ef"
      },
      "source": [
        "Tokenization is the process of breaking down text into smaller chunks, typically words or sentences. This is often the first step in text analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0e9fb7",
      "metadata": {
        "id": "4e0e9fb7"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "text = \"Hello there! Welcome to Principles of Data Science. Hope you find it helpful.\"\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sent_tokenize(text))\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e3d3dd",
      "metadata": {
        "id": "65e3d3dd"
      },
      "source": [
        "4. Part-of-Speech Tagging:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ad60dc",
      "metadata": {
        "id": "56ad60dc"
      },
      "source": [
        "Part-of-Speech (POS) tagging is used to label each word in a sentence as nouns, verbs, adjectives, etc. This is useful for many language processing tasks because it helps in understanding the grammar and role of each word in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577ff9a3",
      "metadata": {
        "id": "577ff9a3"
      },
      "outputs": [],
      "source": [
        "words = word_tokenize(text)\n",
        "print(\"\\nPart-of-Speech Tagging:\")\n",
        "print(nltk.pos_tag(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77f68fc",
      "metadata": {
        "id": "b77f68fc"
      },
      "source": [
        "5. Named Entity Recognition (NER):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fb1940",
      "metadata": {
        "id": "15fb1940"
      },
      "source": [
        "NER is used to identify and classify named entities mentioned in text into pre-defined categories such as person names, organizations, locations, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f96d5f",
      "metadata": {
        "id": "16f96d5f"
      },
      "outputs": [],
      "source": [
        "from nltk import ne_chunk\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "print(ne_chunk(nltk.pos_tag(word_tokenize(text))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba014536",
      "metadata": {
        "id": "ba014536"
      },
      "source": [
        "6. Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1edc8a",
      "metadata": {
        "id": "ba1edc8a"
      },
      "source": [
        "This simple example demonstrates the initial steps in text analysis using NLTK in Python. These foundational techniques are critical in more complex NLP tasks such as sentiment analysis, machine translation, or information extraction. Understanding how to perform basic text processing allows for the exploration and analysis of text data, which can be applied to a vast array of practical applications in business, social media, academia, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7830fe72",
      "metadata": {
        "id": "7830fe72"
      },
      "source": [
        "As you advance, you can explore more sophisticated NLP models and tools, such as those provided by newer libraries like spaCy or transformer-based models like BERT and GPT, which offer pre-trained models capable of understanding and generating human-like text. This can significantly enhance your capability to handle diverse and complex NLP tasks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}