{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_5/Section_4_Python_Example__Using_Pandas_to_Manage_Large_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 4 - Python Example: Using pandas to manage large datasets"
      ],
      "metadata": {
        "id": "UAkt6sxf3wbT"
      },
      "id": "UAkt6sxf3wbT"
    },
    {
      "cell_type": "markdown",
      "id": "8a84b6b6",
      "metadata": {
        "id": "8a84b6b6"
      },
      "source": [
        "Handling large datasets efficiently is crucial in the age of Big Data, where organizations frequently process vast volumes of information. Python's Pandas library is an indispensable tool for data scientists dealing with large datasets, providing powerful data manipulation capabilities that simplify the process of cleaning, transforming, and analysing data. This section illustrates how to use Pandas to manage and analyse large datasets, demonstrating techniques that optimize performance and scalability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5fc0586",
      "metadata": {
        "id": "f5fc0586"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed20b53",
      "metadata": {
        "id": "8ed20b53"
      },
      "source": [
        "To work with Pandas and handle large datasets effectively, ensure your Python environment is set up with the necessary libraries. If Pandas is not installed, you can install it along with Matplotlib for data visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db0ef6a",
      "metadata": {
        "id": "0db0ef6a"
      },
      "outputs": [],
      "source": [
        "pip install pandas matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d073311",
      "metadata": {
        "id": "3d073311"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2483f8a",
      "metadata": {
        "id": "f2483f8a"
      },
      "source": [
        "Start by importing Pandas and other necessary libraries. For performance, you might also consider using libraries like dask for parallel computing or numba for accelerating computation, but for simplicity, this example will focus on Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf66f63",
      "metadata": {
        "id": "acf66f63"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "474aebaa",
      "metadata": {
        "id": "474aebaa"
      },
      "source": [
        "3. Loading Large Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad7b8e4",
      "metadata": {
        "id": "fad7b8e4"
      },
      "source": [
        "Pandas provides various functions to load data from different sources. When dealing with very large datasets, consider loading data in chunks or using iterator options to manage memory usage effectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1840541",
      "metadata": {
        "id": "d1840541"
      },
      "outputs": [],
      "source": [
        "# Example CSV file loading in chunks\n",
        "chunk_size = 10000  # Define the size of each chunk\n",
        "chunks = []  # List to hold chunks of dataframes\n",
        "\n",
        "# Use the chunksize parameter to load data in manageable parts\n",
        "for chunk in pd.read_csv('large_dataset.csv', chunksize=chunk_size):\n",
        "    # Process each chunk during loading if necessary\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# Concatenate chunks into a single DataFrame\n",
        "df = pd.concat(chunks, axis=0)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92723c89",
      "metadata": {
        "id": "92723c89"
      },
      "source": [
        "4. Efficient Data Manipulation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d286af",
      "metadata": {
        "id": "87d286af"
      },
      "source": [
        "Once the data is loaded, efficient manipulation is key. Use vectorized operations provided by Pandas, which are generally faster than applying functions row-wise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371ccb3b",
      "metadata": {
        "id": "371ccb3b"
      },
      "outputs": [],
      "source": [
        "# Example of vectorized operation for a new column creation\n",
        "df['new_column'] = df['existing_column'] * 10  # Simple calculation example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f111298",
      "metadata": {
        "id": "6f111298"
      },
      "source": [
        "5. Filtering and Downsampling Large Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b55af6",
      "metadata": {
        "id": "06b55af6"
      },
      "source": [
        "For extremely large datasets, consider filtering unnecessary data early in the workflow or downsampling to make the dataset more manageable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6000b095",
      "metadata": {
        "id": "6000b095"
      },
      "outputs": [],
      "source": [
        "# Filter rows based on a condition\n",
        "filtered_df = df[df['column_name'] > threshold_value]\n",
        "\n",
        "# Downsampling data to reduce size\n",
        "downsampled_df = df.sample(frac=0.1)  # Retains 10% of the data randomly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cc673db",
      "metadata": {
        "id": "4cc673db"
      },
      "source": [
        "6. Aggregations and Group Operations:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a752db3",
      "metadata": {
        "id": "2a752db3"
      },
      "source": [
        "Use Pandas' efficient aggregation functions to summarize data. Group operations can be memory intensive, so consider grouping by fewer columns and performing aggregations that reduce data size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e650ea",
      "metadata": {
        "id": "36e650ea"
      },
      "outputs": [],
      "source": [
        "# Grouping data and performing an aggregation\n",
        "summary_df = df.groupby('grouping_column').agg({'numeric_column': 'mean'})\n",
        "print(summary_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f758571a",
      "metadata": {
        "id": "f758571a"
      },
      "source": [
        "7. Saving Processed Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99168da9",
      "metadata": {
        "id": "99168da9"
      },
      "source": [
        "After processing, save the processed data to a format that preserves the data type and is easy to load for future analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e423c29c",
      "metadata": {
        "id": "e423c29c"
      },
      "outputs": [],
      "source": [
        "# Save to a CSV file\n",
        "df.to_csv('processed_large_dataset.csv', index=False)\n",
        "\n",
        "# Save to HDF5 format for large numerical data efficient handling\n",
        "df.to_hdf('processed_large_dataset.h5', key='df', mode='w')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3363ca",
      "metadata": {
        "id": "2f3363ca"
      },
      "source": [
        "8. Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e022fb",
      "metadata": {
        "id": "70e022fb"
      },
      "source": [
        "This example demonstrates basic strategies for using Pandas to manage large datasets, focusing on techniques that minimize memory usage and optimize processing speed. Effective use of Pandas for large datasets involves thoughtful consideration of how data is loaded, processed, and stored. With these strategies, data scientists can handle large volumes of data more efficiently, allowing them to focus on extracting meaningful insights rather than struggling with performance issues. As datasets continue to grow in size and complexity, mastering these techniques will become increasingly important."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}