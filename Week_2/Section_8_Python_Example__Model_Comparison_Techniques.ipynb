{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_2/Section_8_Python_Example__Model_Comparison_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 8 Python example - model comparison techniques"
      ],
      "metadata": {
        "id": "MDENWi1D8jCa"
      },
      "id": "MDENWi1D8jCa"
    },
    {
      "cell_type": "markdown",
      "id": "831d534c",
      "metadata": {
        "id": "831d534c"
      },
      "source": [
        "Model comparison is a crucial aspect of the model selection process in data science. It involves evaluating and contrasting the performance of different statistical or machine learning models to determine which one best suits the specific needs of a project. This involves considering not only accuracy but also other performance metrics and characteristics like model simplicity, computation time, and ease of interpretation. In this section, we'll demonstrate how to implement model comparison techniques in Python using the Scikit-learn library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2118b557",
      "metadata": {
        "id": "2118b557"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a582c2e",
      "metadata": {
        "id": "1a582c2e"
      },
      "source": [
        "First, ensure that Python and Scikit-learn are installed in your environment. If Scikit-learn is not installed, you can install it using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c79a739e",
      "metadata": {
        "id": "c79a739e"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65241c8a",
      "metadata": {
        "id": "65241c8a"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6525d764",
      "metadata": {
        "id": "6525d764"
      },
      "source": [
        "Import necessary libraries for data manipulation, model fitting, and evaluation. We'll use Pandas for data handling, Scikit-learn for modeling and metrics, and Matplotlib for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed25d3c2",
      "metadata": {
        "id": "ed25d3c2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3afb7bc",
      "metadata": {
        "id": "f3afb7bc"
      },
      "source": [
        "3. Generating Synthetic Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc38595",
      "metadata": {
        "id": "ffc38595"
      },
      "source": [
        "For this example, letâ€™s create a synthetic dataset suitable for classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7cb86b9",
      "metadata": {
        "id": "d7cb86b9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264d8e75",
      "metadata": {
        "id": "264d8e75"
      },
      "source": [
        "4. Training Models:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d4fd434",
      "metadata": {
        "id": "1d4fd434"
      },
      "source": [
        "We'll compare two commonly used models: Logistic Regression and Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2918d722",
      "metadata": {
        "id": "2918d722"
      },
      "outputs": [],
      "source": [
        "# Initialize the models\n",
        "model_lr = LogisticRegression()\n",
        "model_rf = RandomForestClassifier()\n",
        "\n",
        "# Fit the models\n",
        "model_lr.fit(X_train, y_train)\n",
        "model_rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0427d9cb",
      "metadata": {
        "id": "0427d9cb"
      },
      "source": [
        "5. Evaluating and Comparing Models:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "756cbc66",
      "metadata": {
        "id": "756cbc66"
      },
      "source": [
        "Use cross-validation and ROC curves to evaluate and compare these models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a0723e",
      "metadata": {
        "id": "e9a0723e"
      },
      "outputs": [],
      "source": [
        "# Perform cross-validation\n",
        "scores_lr = cross_val_score(model_lr, X_train, y_train, cv=5, scoring='accuracy')\n",
        "scores_rf = cross_val_score(model_rf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Average accuracy for Logistic Regression: {:.2f}%\".format(np.mean(scores_lr) * 100))\n",
        "print(\"Average accuracy for Random Forest: {:.2f}%\".format(np.mean(scores_rf) * 100))\n",
        "\n",
        "# Compute ROC AUC scores\n",
        "roc_auc_lr = roc_auc_score(y_test, model_lr.predict_proba(X_test)[:, 1])\n",
        "roc_auc_rf = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
        "\n",
        "print(\"ROC AUC for Logistic Regression: {:.2f}\".format(roc_auc_lr))\n",
        "print(\"ROC AUC for Random Forest: {:.2f}\".format(roc_auc_rf))\n",
        "\n",
        "# Generate ROC curves\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, model_lr.predict_proba(X_test)[:, 1])\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_lr, tpr_lr, label='Logistic Regression (area = {:.2f})'.format(roc_auc_lr))\n",
        "plt.plot(fpr_rf, tpr_rf, label='Random Forest (area = {:.2f})'.format(roc_auc_rf))\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb23377",
      "metadata": {
        "id": "bcb23377"
      },
      "source": [
        "6. Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5fc550c",
      "metadata": {
        "id": "b5fc550c"
      },
      "source": [
        "The above example illustrates how to perform model comparison using cross-validation and ROC curves, two powerful techniques for assessing model performance. While accuracy gives a quick snapshot of model effectiveness, ROC curves and AUC scores provide deeper insights into model behaviour across different classification thresholds. These techniques help in making an informed choice about which model to deploy based on the project's specific requirements."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}