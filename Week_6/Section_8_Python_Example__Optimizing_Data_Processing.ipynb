{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_6/Section_8_Python_Example__Optimizing_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 8 - Python example: optimizing data processing"
      ],
      "metadata": {
        "id": "b5AWUk0pfxjj"
      },
      "id": "b5AWUk0pfxjj"
    },
    {
      "cell_type": "markdown",
      "id": "4922bbd9",
      "metadata": {
        "id": "4922bbd9"
      },
      "source": [
        "In data science, optimizing data processing can significantly enhance the performance and scalability of data analysis workflows. This section demonstrates practical Python techniques for optimizing data processing tasks using libraries such as Pandas, NumPy, and Dask. These examples highlight methods to improve computational efficiency, manage memory usage effectively, and reduce processing time, especially when dealing with large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c7f60c",
      "metadata": {
        "id": "a4c7f60c"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc53875",
      "metadata": {
        "id": "7dc53875"
      },
      "source": [
        "Before diving into data processing optimization, ensure your Python environment is equipped with the necessary libraries. If Pandas, NumPy, or Dask are not installed, they can be added using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3d13d17c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d13d17c",
        "outputId": "623a6da8-80fc-49ff-ce17-7591831b52ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (2024.8.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask) (24.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask) (3.20.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy dask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17cf393f",
      "metadata": {
        "id": "17cf393f"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18db4d90",
      "metadata": {
        "id": "18db4d90"
      },
      "source": [
        "Start by importing the libraries that will be used throughout the examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4e31fc07",
      "metadata": {
        "id": "4e31fc07"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.array as da"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33b830d",
      "metadata": {
        "id": "c33b830d"
      },
      "source": [
        "3. Using Vectorization in NumPy:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f21cc55",
      "metadata": {
        "id": "3f21cc55"
      },
      "source": [
        "Vectorization is a powerful method for minimizing loop usage and optimizing computations. Hereâ€™s how you can utilize NumPy for vectorized operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "95570324",
      "metadata": {
        "id": "95570324"
      },
      "outputs": [],
      "source": [
        "# Create large numpy arrays\n",
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000)\n",
        "# Vectorized addition\n",
        "result = a + b\n",
        "# Much faster than iterating through arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b55fb3",
      "metadata": {
        "id": "25b55fb3"
      },
      "source": [
        "4. Efficient Data Loading and Processing with Pandas:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab098db",
      "metadata": {
        "id": "aab098db"
      },
      "source": [
        "Handling large datasets efficiently in Pandas involves optimizing how data is loaded and manipulated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a65593bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "a65593bf",
        "outputId": "1f7faab0-337e-4769-fb9f-645dd8a21262"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'large_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b4fb92880182>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read a large CSV file in chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'large_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Process each chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_column'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'existing_column'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# Process data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'large_dataset.csv'"
          ]
        }
      ],
      "source": [
        "# Read a large CSV file in chunks\n",
        "iterator = pd.read_csv('large_dataset.csv', chunksize=10000)\n",
        "# Process each chunk\n",
        "for chunk in iterator:\n",
        "    chunk['new_column'] = chunk['existing_column'] * 10 # Process data\n",
        "# Save or aggregate results here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ee67d0",
      "metadata": {
        "id": "67ee67d0"
      },
      "source": [
        "5. Parallel Processing with Dask:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f37c2e",
      "metadata": {
        "id": "47f37c2e"
      },
      "source": [
        "Dask provides advanced parallel computing capabilities, making it ideal for working with large data sets efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b63151",
      "metadata": {
        "id": "15b63151"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "# Create a Dask DataFrame from a Pandas DataFrame\n",
        "dask_df = dd.from_pandas(pd.DataFrame({'x': range(100000), 'y': range(100000)}), npartitions=10)\n",
        "# Perform operations in parallel\n",
        "result = dask_df.x + dask_df.y\n",
        "# This operation is lazy and computed in parallel\n",
        "computed_result = result.compute() # Trigger computation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68955faf",
      "metadata": {
        "id": "68955faf"
      },
      "source": [
        "6. Memory Management:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ed044d",
      "metadata": {
        "id": "91ed044d"
      },
      "source": [
        "Effective memory management is crucial for handling large datasets. Techniques such as using smaller data types and cleaning up dataframes can help:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1d4c96",
      "metadata": {
        "id": "3a1d4c96"
      },
      "outputs": [],
      "source": [
        "# Optimize data types\n",
        "df = pd.DataFrame({'A': pd.Series(np.random.randint(1, 100, size=1000000))}) print(df['A'].memory_usage(deep=True), 'bytes') df['A'] = df['A'].astype('int8') print(df['A'].memory_usage(deep=True), 'bytes') # Memory usage is reduced # Explicitly delete dataframes when they're no longer needed del df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b258f0ea",
      "metadata": {
        "id": "b258f0ea"
      },
      "source": [
        "7. Utilizing Efficient File Formats:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050b139e",
      "metadata": {
        "id": "050b139e"
      },
      "source": [
        "Using efficient file formats can speed up read and write operations significantly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8d7538",
      "metadata": {
        "id": "2f8d7538"
      },
      "outputs": [],
      "source": [
        "# Using HDF5 format\n",
        "df = pd.DataFrame({'A': np.random.rand(1000000)})\n",
        "df.to_hdf('data.h5', key='df', mode='w')\n",
        "# Fast loading\n",
        "df = pd.read_hdf('data.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2282539",
      "metadata": {
        "id": "d2282539"
      },
      "source": [
        "Optimizing data processing tasks in Python involves a combination of efficient coding practices, leveraging powerful libraries like Pandas, NumPy, and Dask, and employing strategies for effective memory management and parallel processing. By implementing these techniques, data scientists can handle larger datasets more effectively, perform faster analyses, and scale their data processing workflows to meet the demands of increasingly data-intensive applications. These optimizations not only save time but also reduce computational costs, making them indispensable in modern data science."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}