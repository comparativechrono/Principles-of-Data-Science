{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_6/Section_8_Python_Example__Optimizing_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 8 - Python example: optimizing data processing"
      ],
      "metadata": {
        "id": "b5AWUk0pfxjj"
      },
      "id": "b5AWUk0pfxjj"
    },
    {
      "cell_type": "markdown",
      "id": "4922bbd9",
      "metadata": {
        "id": "4922bbd9"
      },
      "source": [
        "In data science, optimizing data processing can significantly enhance the performance and scalability of data analysis workflows. This section demonstrates practical Python techniques for optimizing data processing tasks using libraries such as Pandas, NumPy, and Dask. These examples highlight methods to improve computational efficiency, manage memory usage effectively, and reduce processing time, especially when dealing with large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c7f60c",
      "metadata": {
        "id": "a4c7f60c"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc53875",
      "metadata": {
        "id": "7dc53875"
      },
      "source": [
        "Before diving into data processing optimization, ensure your Python environment is equipped with the necessary libraries. If Pandas, NumPy, or Dask are not installed, they can be added using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d13d17c",
      "metadata": {
        "id": "3d13d17c"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy dask time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17cf393f",
      "metadata": {
        "id": "17cf393f"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18db4d90",
      "metadata": {
        "id": "18db4d90"
      },
      "source": [
        "Start by importing the libraries that will be used throughout the examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e31fc07",
      "metadata": {
        "id": "4e31fc07"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.array as da\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33b830d",
      "metadata": {
        "id": "c33b830d"
      },
      "source": [
        "3. Using Vectorization in NumPy:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f21cc55",
      "metadata": {
        "id": "3f21cc55"
      },
      "source": [
        "Vectorization is a powerful method for minimizing loop usage and optimizing computations. Here’s how you can utilize NumPy for vectorized operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95570324",
      "metadata": {
        "id": "95570324"
      },
      "outputs": [],
      "source": [
        "# Create large numpy arrays\n",
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000)\n",
        "# Vectorized addition\n",
        "result = a + b\n",
        "# Much faster than iterating through arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b55fb3",
      "metadata": {
        "id": "25b55fb3"
      },
      "source": [
        "4. Efficient Data Loading and Processing with Pandas:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab098db",
      "metadata": {
        "id": "aab098db"
      },
      "source": [
        "Handling large datasets efficiently in Pandas involves optimizing how data is loaded and manipulated. When loading large files it is helpful to consider what we would do if the file was too big for our system. A good option is to chunk the file, although this can take longer, it is a necessary trade-off. Here is an example of how we can use the time module to look at the loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a65593bf",
      "metadata": {
        "id": "a65593bf"
      },
      "outputs": [],
      "source": [
        "# Option 1: Attempt to load the entire large file at once\n",
        "try:\n",
        "    print(\"Attempting to load the entire file at once...\")\n",
        "    start_time = time.time()\n",
        "    large_df = pd.read_csv('large_dataset.csv')\n",
        "    load_time_entire = time.time() - start_time\n",
        "    print(f\"Loaded entire file successfully in {load_time_entire:.2f} seconds. Shape:\", large_df.shape)\n",
        "except MemoryError:\n",
        "    print(\"MemoryError: The file is too large to load into memory all at once.\")\n",
        "\n",
        "# Option 2: Load the file in chunks and measure the time taken\n",
        "print(\"\\nProcessing the file in chunks...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Read the large CSV file in chunks\n",
        "iterator = pd.read_csv('large_dataset.csv', chunksize=10000)\n",
        "\n",
        "# Process each chunk\n",
        "for i, chunk in enumerate(iterator):\n",
        "    # Process data within each chunk\n",
        "    chunk['new_column'] = chunk['existing_column'] * 10\n",
        "\n",
        "    # Optionally display the first processed chunk\n",
        "    if i == 0:\n",
        "        print(\"Sample of the first processed chunk:\")\n",
        "        print(chunk.head())  # Display the first few rows as an example\n",
        "\n",
        "# Calculate time taken for chunk processing\n",
        "load_time_chunks = time.time() - start_time\n",
        "print(f\"\\nChunk processing complete in {load_time_chunks:.2f} seconds.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ee67d0",
      "metadata": {
        "id": "67ee67d0"
      },
      "source": [
        "5. Parallel Processing with Dask:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f37c2e",
      "metadata": {
        "id": "47f37c2e"
      },
      "source": [
        "Dask provides advanced parallel computing capabilities, making it ideal for working with large data sets efficiently. We'll look at this again in section 10. In this section, let's learn how to make a dask DataFrame for performing parallel operations:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"dask[dataframe]\"\n"
      ],
      "metadata": {
        "id": "6Y9z0c_r0uNt"
      },
      "id": "6Y9z0c_r0uNt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b63151",
      "metadata": {
        "id": "15b63151"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Create a Dask DataFrame from a Pandas DataFrame\n",
        "dask_df = dd.from_pandas(pd.DataFrame({'x': range(100000), 'y': range(100000)}), npartitions=10)\n",
        "# Perform operations in parallel\n",
        "result = dask_df.x + dask_df.y\n",
        "# This operation is lazy and computed in parallel\n",
        "computed_result = result.compute() # Trigger computation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How could we use the time module to measure the compute time of this operation? Have a go at adapting the previous example that used time to look at the speed of running tasks in parallel with dask. Is dask quicker?"
      ],
      "metadata": {
        "id": "De6osi3L2JTz"
      },
      "id": "De6osi3L2JTz"
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "sRqeNFP_2_LE"
      },
      "id": "sRqeNFP_2_LE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "68955faf",
      "metadata": {
        "id": "68955faf"
      },
      "source": [
        "6. Memory Management:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ed044d",
      "metadata": {
        "id": "91ed044d"
      },
      "source": [
        "Effective memory management is crucial for handling large datasets. Techniques such as using smaller data types and cleaning up dataframes can help:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1d4c96",
      "metadata": {
        "id": "3a1d4c96"
      },
      "outputs": [],
      "source": [
        "# Optimize data types\n",
        "df = pd.DataFrame({'A': pd.Series(np.random.randint(1, 100, size=1000000))})\n",
        "\n",
        "# Print memory usage before optimization\n",
        "print(df['A'].memory_usage(deep=True), 'bytes')\n",
        "\n",
        "# Convert the column to a smaller integer type to save memory\n",
        "df['A'] = df['A'].astype('int8')\n",
        "\n",
        "# Print memory usage after optimization\n",
        "print(df['A'].memory_usage(deep=True), 'bytes')  # Memory usage is reduced\n",
        "\n",
        "# Explicitly delete dataframes when they're no longer needed\n",
        "del df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b258f0ea",
      "metadata": {
        "id": "b258f0ea"
      },
      "source": [
        "7. Utilizing Efficient File Formats:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050b139e",
      "metadata": {
        "id": "050b139e"
      },
      "source": [
        "Using efficient file formats can speed up read and write operations significantly. HDF5 is a modern format that can greatly increase efficiency when dealing with big data. This can still be loaded into pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8d7538",
      "metadata": {
        "id": "2f8d7538"
      },
      "outputs": [],
      "source": [
        "# Using HDF5 format\n",
        "df = pd.DataFrame({'A': np.random.rand(1000000)})\n",
        "df.to_hdf('data.h5', key='df', mode='w')\n",
        "# Fast loading\n",
        "df = pd.read_hdf('data.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2282539",
      "metadata": {
        "id": "d2282539"
      },
      "source": [
        "Optimizing data processing tasks in Python involves a combination of efficient coding practices, leveraging powerful libraries like Pandas, NumPy, and Dask, and employing strategies for effective memory management and parallel processing. By implementing these techniques, data scientists can handle larger datasets more effectively, perform faster analyses, and scale their data processing workflows to meet the demands of increasingly data-intensive applications. These optimizations not only save time but also reduce computational costs, making them indispensable in modern data science."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we’ve explored how Pandas and Dask handle large datasets and why Dask is often chosen for \"big data\" operations. By examining each approach through the lens of Big O notation, we can better understand the theoretical efficiency and trade-offs involved.\n",
        "\n",
        "When loading a dataset and performing operations such as addition on columns, Pandas operates with **linear time complexity, \\( O(N) \\)**. This means that as the dataset size \\( N \\) grows, the time required to load and process the dataset scales linearly. Pandas is efficient for datasets that fit in memory, as it doesn’t introduce the overhead required for parallel processing, making it the faster option in this case.\n",
        "\n",
        "Dask, on the other hand, also has **linear complexity** for basic operations, but it introduces additional overhead due to partitioning the dataset and managing parallel tasks. The overall complexity of Dask is **\\( O(N) + O(P) \\)**, where \\( P \\) represents the number of partitions. Although this added overhead can make Dask slower for smaller datasets, it’s a necessary trade-off for managing larger-than-memory datasets. By partitioning the dataset and processing each partition in parallel, Dask makes it possible to work with massive datasets that would otherwise be infeasible in Pandas alone.\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Approach        | Time Complexity | Practical Use Case                                 |\n",
        "|-----------------|-----------------|----------------------------------------------------|\n",
        "| **Pandas**      | \\( O(N) \\)      | Best for datasets that fit comfortably in memory, where linear processing can happen without parallel overhead. |\n",
        "| **Dask**        | \\( O(N) + O(P) \\) | Best for large, memory-intensive datasets, as Dask's partitioning and parallel processing enable scalable operations, despite some overhead.|\n",
        "\n",
        "This comparison highlights that **Dask doesn’t necessarily reduce complexity** but instead provides a way to handle large data efficiently by using parallelism. Through this analysis, we see how Big O notation not only reveals the efficiency of an algorithm but also highlights trade-offs related to practical memory constraints and computational scalability."
      ],
      "metadata": {
        "id": "YC1MBVYx3cl3"
      },
      "id": "YC1MBVYx3cl3"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}