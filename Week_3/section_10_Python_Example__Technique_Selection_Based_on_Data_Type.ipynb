{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_3/section_10_Python_Example__Technique_Selection_Based_on_Data_Type.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 10- Technique selection based on data type"
      ],
      "metadata": {
        "id": "JUM6TmcMewDs"
      },
      "id": "JUM6TmcMewDs"
    },
    {
      "cell_type": "markdown",
      "id": "6cb5ea03",
      "metadata": {
        "id": "6cb5ea03"
      },
      "source": [
        "Selecting the appropriate data analysis technique based on the type of data at hand is crucial for extracting meaningful insights and achieving accurate results. This section provides a practical example using Python to demonstrate how to select and apply different analysis techniques depending on whether the data is numeric, categorical, or text-based. We will utilize Python's rich ecosystem, including libraries like scikit-learn, pandas, and nltk, to handle various data types effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - revision of the techniques covered this week"
      ],
      "metadata": {
        "id": "dy6SNuLOfBA-"
      },
      "id": "dy6SNuLOfBA-"
    },
    {
      "cell_type": "markdown",
      "id": "0dc7dbc5",
      "metadata": {
        "id": "0dc7dbc5"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df5d9dc",
      "metadata": {
        "id": "3df5d9dc"
      },
      "source": [
        "Before starting, ensure Python and the necessary libraries are installed. If not, install them using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190b44b5",
      "metadata": {
        "id": "190b44b5"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas scikit-learn matplotlib nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99370467",
      "metadata": {
        "id": "99370467"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34443d63",
      "metadata": {
        "id": "34443d63"
      },
      "source": [
        "Import the libraries that will be used to handle different data types and perform data analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23843a36",
      "metadata": {
        "id": "23843a36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64528299",
      "metadata": {
        "id": "64528299"
      },
      "source": [
        "3. Preparing Different Data Types:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee70dd2",
      "metadata": {
        "id": "aee70dd2"
      },
      "source": [
        "We'll create synthetic examples for three types of data: numeric, categorical, and text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183a63d5",
      "metadata": {
        "id": "183a63d5"
      },
      "outputs": [],
      "source": [
        "# Numeric Data: Generating a simple dataset with height (cm) and weight (kg)\n",
        "np.random.seed(0)\n",
        "data_numeric = pd.DataFrame({\n",
        "    'Height': np.random.normal(loc=170, scale=10, size=100),\n",
        "    'Weight': np.random.normal(loc=65, scale=15, size=100)\n",
        "})\n",
        "\n",
        "# Categorical Data: Creating a dataset with 'Gender' and 'Product' categories\n",
        "data_categorical = pd.DataFrame({\n",
        "    'Gender': ['Male', 'Female'] * 50,\n",
        "    'Product': np.random.choice(['Product A', 'Product B', 'Product C'], 100)\n",
        "})\n",
        "\n",
        "# Text Data: Sample sentences for sentiment analysis\n",
        "data_text = pd.Series([\n",
        "    \"I love this product\",\n",
        "    \"This is the worst experience of my life\",\n",
        "    \"I feel great about this\",\n",
        "    \"This is not good\",\n",
        "    \"Absolutely fantastic!\"\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e3ad38",
      "metadata": {
        "id": "79e3ad38"
      },
      "source": [
        "4. Technique for Numeric Data: Standard Scaling and Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ab1feb",
      "metadata": {
        "id": "98ab1feb"
      },
      "source": [
        "For numeric data, we'll scale the data and apply a Gaussian Naive Bayes classifier to predict a simple outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9769f893",
      "metadata": {
        "id": "9769f893"
      },
      "outputs": [],
      "source": [
        "# Scale the numeric data\n",
        "scaler = StandardScaler()\n",
        "data_numeric_scaled = scaler.fit_transform(data_numeric)\n",
        "\n",
        "# Assuming a binary target variable\n",
        "target = np.random.choice([0, 1], size=100)\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_numeric_scaled, target, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(\"Accuracy on numeric data:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08092adc",
      "metadata": {
        "id": "08092adc"
      },
      "source": [
        "5. Technique for Categorical Data: Label Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3086b531",
      "metadata": {
        "id": "3086b531"
      },
      "source": [
        "For categorical data, we'll use label encoding to convert categories into integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e20fd4d",
      "metadata": {
        "id": "0e20fd4d"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "data_categorical_encoded = data_categorical.apply(encoder.fit_transform)\n",
        "\n",
        "# Display the encoded data\n",
        "print(data_categorical_encoded.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36cf94e3",
      "metadata": {
        "id": "36cf94e3"
      },
      "source": [
        "6. Technique for Text Data: Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277a8173",
      "metadata": {
        "id": "277a8173"
      },
      "source": [
        "Using NLTK for sentiment analysis on text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9f04c1",
      "metadata": {
        "id": "bf9f04c1"
      },
      "outputs": [],
      "source": [
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Applying sentiment analysis\n",
        "data_text_sentiment = data_text.apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "\n",
        "# Display sentiment scores\n",
        "print(data_text_sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 2 - EDA for technique selection"
      ],
      "metadata": {
        "id": "X1CWiQD9e5G0"
      },
      "id": "X1CWiQD9e5G0"
    },
    {
      "cell_type": "markdown",
      "id": "8f9976f5",
      "metadata": {
        "id": "8f9976f5"
      },
      "source": [
        "Choosing the correct data analysis technique based on data type is crucial for achieving accurate and meaningful insights. Now we will demonstrate a Python example that outlines how to programmatically identify data types within a dataset and select appropriate analysis techniques accordingly. We'll use Python's pandas library for data manipulation, matplotlib for visualization, and scikit-learn for implementing different machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c204b32",
      "metadata": {
        "id": "7c204b32"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6b6d69",
      "metadata": {
        "id": "5c6b6d69"
      },
      "source": [
        "First, ensure you have Python installed with the necessary libraries. If not, install them using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e8146d",
      "metadata": {
        "id": "c7e8146d"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas matplotlib scikit-learn seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c74838f",
      "metadata": {
        "id": "5c74838f"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035d2b0c",
      "metadata": {
        "id": "035d2b0c"
      },
      "source": [
        "Import the libraries that will be used for data handling, visualization, and analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7777e1",
      "metadata": {
        "id": "bd7777e1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17446a66",
      "metadata": {
        "id": "17446a66"
      },
      "source": [
        "3. Preparing a Sample Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135c8dfe",
      "metadata": {
        "id": "135c8dfe"
      },
      "source": [
        "For illustration, we'll create a sample DataFrame with mixed data types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4cd8b7",
      "metadata": {
        "id": "3a4cd8b7"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with mixed data types\n",
        "data = pd.DataFrame({\n",
        "    'CustomerID': range(1, 101),\n",
        "    'Age': np.random.randint(18, 70, size=100),\n",
        "    'Income': np.random.normal(50000, 12000, size=100),\n",
        "    'Gender': np.random.choice(['Male', 'Female'], size=100),\n",
        "    'TextFeedback': np.random.choice(['Good', 'Bad', 'Neutral'], size=100)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b558a09",
      "metadata": {
        "id": "7b558a09"
      },
      "source": [
        "4. Analyzing Data Types and Selecting Techniques:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a265aa1",
      "metadata": {
        "id": "7a265aa1"
      },
      "source": [
        "We will programmatically check data types and plot pairwise relationships for numeric data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdaad72c",
      "metadata": {
        "id": "bdaad72c"
      },
      "outputs": [],
      "source": [
        "# Analyze data types\n",
        "data_types = data.dtypes\n",
        "\n",
        "# Print data types\n",
        "print(data_types)\n",
        "\n",
        "# Select numerical columns for pairwise plot\n",
        "numerical_cols = data.select_dtypes(include=[np.number])\n",
        "\n",
        "# Use seaborn to plot pairwise relationships\n",
        "sns.pairplot(numerical_cols)\n",
        "plt.show()\n",
        "\n",
        "# For categorical data, we could use label encoding or one-hot encoding\n",
        "# Here we decide based on number of unique categories\n",
        "if data['Gender'].nunique() <= 2:\n",
        "    data['Gender_encoded'] = data['Gender'].map({'Male': 0, 'Female': 1})\n",
        "else:\n",
        "    data = pd.get_dummies(data, columns=['Gender'])\n",
        "\n",
        "# Decision based on whether to apply clustering or classification\n",
        "if 'Outcome' in data.columns:\n",
        "    # Assume 'Outcome' is a binary classification target\n",
        "    X = data.drop('Outcome', axis=1)\n",
        "    y = data['Outcome']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Classification model trained.\")\n",
        "else:\n",
        "    # Apply clustering if no apparent target variable is defined\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(numerical_cols)\n",
        "    kmeans = KMeans(n_clusters=3)\n",
        "    clusters = kmeans.fit_predict(scaled_data)\n",
        "    print(\"Clustering performed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stop - where has our data gone?"
      ],
      "metadata": {
        "id": "oTBnOyY9g6RA"
      },
      "id": "oTBnOyY9g6RA"
    },
    {
      "cell_type": "markdown",
      "id": "59fc6125",
      "metadata": {
        "id": "59fc6125"
      },
      "source": [
        "This example has shown how to programmatically inspect a dataset to determine the data types and choose appropriate analysis techniques based on the characteristics of the data. By integrating such logic, Python scripts can be made more flexible and adaptable to varying datasets. Properly identifying the type and structure of data before applying any analysis techniques ensures that the methods chosen are suitable and that the insights generated are both reliable and actionable. However, you will have noted that in the seaborn pairs plot above we were only able to include those variables that are numerical. This is one point where R has the advantage on python, with its built in pairs() function, which seamlessly handles mixed data types. To solve this problem we will combine what we have covered in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dfef558",
      "metadata": {
        "id": "1dfef558"
      },
      "source": [
        "In Python, creating a comprehensive pairs plot that includes both numerical and categorical data is possible but requires a bit more manual work compared to R's pairs() function. A similar approach can be taken to handle categorical data in a pairs plot by manually converting categorical variables to numerical format before plotting. This can be done using encoding techniques such as label encoding, where each category is assigned a unique integer. Python's seaborn library provides a function called pairplot which is typically used for plotting pairwise relationships in a dataset but is primarily designed for numerical data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d5bd86",
      "metadata": {
        "id": "f1d5bd86"
      },
      "source": [
        "Letâ€™s modify the previous example with the addition of converting categorical variables to numerical and use seaborn.pairplot to create a pairs plot in Python:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f81d01",
      "metadata": {
        "id": "a8f81d01"
      },
      "source": [
        "4. Converting Categorical Data to Numeric: Use label encoding to convert categorical variables into numbers. This can be achieved using LabelEncoder from sklearn.preprocessing or with Pandas functionalities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb39570",
      "metadata": {
        "id": "ceb39570"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply label encoder for each categorical column\n",
        "data['Gender'] = label_encoder.fit_transform(data['Gender'])\n",
        "data['TextFeedback'] = label_encoder.fit_transform(data['TextFeedback'])\n",
        "\n",
        "# Now all data is numeric\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9189b43f",
      "metadata": {
        "id": "9189b43f"
      },
      "source": [
        "5. Generating the Pair Plot: Now that all data is numeric, you can easily generate a pairs plot using seaborn.pairplot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4444fc85",
      "metadata": {
        "id": "4444fc85"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ea0524",
      "metadata": {
        "id": "83ea0524"
      },
      "source": [
        "This plot will include all variables in the data, showing scatter plots for all pairs of numerical data and histograms for the distribution of each single variable. Do you think this was more useful? Are there any better approaches to EDA?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Over to you..."
      ],
      "metadata": {
        "id": "3TEzxEgEfk5r"
      },
      "id": "3TEzxEgEfk5r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please analyse a dataset of your choice using the techniques we have discussed. You coudl use one of the datasets we have already encountered in this course, or explore one of the build in data sets in Scikit learn - we have explored how to look at these in todays pattern reocognition notebook."
      ],
      "metadata": {
        "id": "MoORh2QBfn_J"
      },
      "id": "MoORh2QBfn_J"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}