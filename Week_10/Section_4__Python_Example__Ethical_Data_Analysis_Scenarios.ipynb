{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comparativechrono/Principles-of-Data-Science/blob/main/Week_10/Section_4__Python_Example__Ethical_Data_Analysis_Scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 4: Python example - ethical data analysis scenarios"
      ],
      "metadata": {
        "id": "jX7HCyf_dEdv"
      },
      "id": "jX7HCyf_dEdv"
    },
    {
      "cell_type": "markdown",
      "id": "8e21aaf3",
      "metadata": {
        "id": "8e21aaf3"
      },
      "source": [
        "In this section, we explore how to handle ethical dilemmas in data analysis through practical Python examples. These scenarios demonstrate the importance of ethical decision-making in data science, focusing on data anonymization, bias detection, and ethical modeling practices. These Python examples will provide a glimpse into the implementation of ethical practices in the data analysis workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605b3a4b",
      "metadata": {
        "id": "605b3a4b"
      },
      "source": [
        "1. Setting Up the Environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8802c14b",
      "metadata": {
        "id": "8802c14b"
      },
      "source": [
        "For these examples, ensure your Python environment includes libraries that support data manipulation and ethical modeling practices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8141ba1",
      "metadata": {
        "id": "d8141ba1"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd2cd89",
      "metadata": {
        "id": "7bd2cd89"
      },
      "source": [
        "2. Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c06bbb",
      "metadata": {
        "id": "20c06bbb"
      },
      "source": [
        "We'll use Pandas for data manipulation, NumPy for numerical operations, and scikit-learn for modeling and bias detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a72c0a7b",
      "metadata": {
        "id": "a72c0a7b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7425d472",
      "metadata": {
        "id": "7425d472"
      },
      "source": [
        "3. Scenario 1: Data Anonymization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126e761c",
      "metadata": {
        "id": "126e761c"
      },
      "source": [
        "Anonymization helps protect personal information in datasets. In this example, we anonymize a dataset by removing direct identifiers and shuffling data to prevent de-anonymization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42cc1c05",
      "metadata": {
        "id": "42cc1c05"
      },
      "outputs": [],
      "source": [
        "# Sample Data Creation\n",
        "data = pd.DataFrame({ 'Name': ['Alice', 'Bob', 'Charlie', 'David'], 'Age': [25, 30, 35, 40], 'Medical Condition': ['Diabetes', 'Healthy', 'Heart Disease', 'Healthy'] })\n",
        "\n",
        "# Removing identifiable information\n",
        "data.drop('Name', axis=1, inplace=True)\n",
        "\n",
        "# Shuffling rows to anonymize the data further\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e341ec2",
      "metadata": {
        "id": "7e341ec2"
      },
      "source": [
        "4. Scenario 2: Detecting and Mitigating Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48523c1",
      "metadata": {
        "id": "a48523c1"
      },
      "source": [
        "Bias in datasets can lead to unfair outcomes. Here, we simulate a scenario where a dataset might have gender bias, and we demonstrate how to detect and address it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "832a4217",
      "metadata": {
        "id": "832a4217"
      },
      "outputs": [],
      "source": [
        "# Generating synthetic data\n",
        "np.random.seed(0)\n",
        "data = pd.DataFrame({\n",
        "    'Gender': ['Male'] * 50 + ['Female'] * 50,\n",
        "    'Hours_Studied': np.random.normal(30, 10, 100),\n",
        "    'Exam_Score': np.random.normal(75, 15, 100)\n",
        "})\n",
        "\n",
        "# Encoding categorical data\n",
        "data['Gender'] = LabelEncoder().fit_transform(data['Gender'])\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[['Gender', 'Hours_Studied']],\n",
        "    data['Exam_Score'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Model Training\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Detecting bias: Checking if the model score is significantly different across groups\n",
        "male_indices = X_test['Gender'] == 1\n",
        "female_indices = X_test['Gender'] == 0\n",
        "\n",
        "male_pred = model.predict(X_test[male_indices])\n",
        "female_pred = model.predict(X_test[female_indices])\n",
        "\n",
        "male_actual = y_test[male_indices]\n",
        "female_actual = y_test[female_indices]\n",
        "\n",
        "male_score = r2_score(male_actual, male_pred)\n",
        "female_score = r2_score(female_actual, female_pred)\n",
        "\n",
        "print(f'Male Score: {male_score}, Female Score: {female_score}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed01a05",
      "metadata": {
        "id": "4ed01a05"
      },
      "source": [
        "5. Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0707bb",
      "metadata": {
        "id": "1b0707bb"
      },
      "source": [
        "These examples highlight the importance of ethical considerations in data science projects. Anonymizing data helps protect individual privacy, while detecting and mitigating bias ensures fair and equitable outcomes from machine learning models. By integrating these practices into data science workflows, practitioners can uphold ethical standards and foster trust in their applications. This proactive approach to ethics should be a continuous process, evolving with new discoveries and societal norms in the field of data science."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}